# ECE368
Probabilistic Reasoning | Machine Learning

<br>
<h3>Lab 1 Classification with Multinomial and Gaussian Models</h3>

The first part of this lab involved designing a **Naives Bayes classifier** to implement a spam-filtration system, flagging emails as "spam" or "ham (not-spam)" based on whether and how many times a certain list of words appeared in the email. Using a **multinomial distribution** for the feature vector and a **maximum likelihood estimate** with **Laplace smoothing** for the probabilities, expressions for the likelihood function and conditional and posterior probabilities were derived and coded from scratch in Python. While the final model had [...]% and [...]% as its **false negative** (Type 1 error) **false positive** (Type 2 error) rates, respectively, the lab also introduced the concept of adding an extra term to the decision rule to control the trade-off between the two types of errors. 

The second part of this lab also involved binary classification with one key difference from the first part being the involvement of feature vectors containing real values. This required the use of a **Gaussian Vector Model**, and through the use of **linear discriminant analysis (LDA)** and **quadratic discriminant analysis (QDA)**, height and weight data was used to predict whether an individual belonged to the "male" or "female" class. The classifications were then visualized on a 2D-plot showing the respective linear and quadratic boundaries.

<h3>Lab 2 Bayesian Linear Regression</h3>

<p>[...]</p>

<h3>Lab 3 Hidden Markov Models</h3>

<p>[...]</p>
